{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ FILE\n",
    "\n",
    "sent=[]\n",
    "just_sent=[]\n",
    "label=[]\n",
    "meta1=[]\n",
    "meta2=[]\n",
    "meta3=[]\n",
    "meta4=[]\n",
    "meta5=[]\n",
    "with open('./dataset/train2.tsv') as f:\n",
    "    lines=f.readlines()\n",
    "    for i in lines:\n",
    "        i=i.strip('\\n')\n",
    "        i=i.split('\\t')\n",
    "        if len(i[2].split(' '))<=2:\n",
    "            label.append(i[2])\n",
    "            sent.append(i[3])\n",
    "            meta1.append(i[4])\n",
    "            meta2.append(i[5])\n",
    "            meta3.append(i[6])\n",
    "            meta4.append(i[7])\n",
    "            meta5.append(i[-2])\n",
    "            just_sent.append(i[-1])\n",
    "#print(sent[0],just_sent[0],label[0],meta1[0],meta2[0],meta3[0],meta4[0],meta5[0])\n",
    "# for i in range(len(label)):\n",
    "#     print(i,label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words and punctuation followed by stemming\n",
    "\n",
    "## TRY without stemming\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "ps=SnowballStemmer('english')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter(sentence):\n",
    "    word_tokens=word_tokenize(sentence)\n",
    "    word_tokens = [w.lower() for w in word_tokens if not (w in stop_words or w in string.punctuation)]\n",
    "    filtered_sentence = [ps.stem(w) for w in word_tokens]\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter(\"This is a sample sentence, showing off the stop words filtration 10 .\")\n",
    "filter_sent=[]\n",
    "filter_just_sent=[]\n",
    "for i in range(len(label)):\n",
    "    filter_sent.append(filter(sent[i]))\n",
    "    filter_just_sent.append(filter(just_sent[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "meta1=[word_tokenize(w.lower()) for w in meta1]\n",
    "meta2=[word_tokenize(w.lower()) for w in meta2]\n",
    "meta3=[word_tokenize(w.lower()) for w in meta5]\n",
    "meta4=[word_tokenize(w.lower()) for w in meta4]\n",
    "meta5=[word_tokenize(w.lower()) for w in meta5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# embed = hub.Module(\"./sentence_wise_email/module/module_useT\")\n",
    "# sent_messages = [' '.join(i) for i in filter_sent]# Reduce logging output.\n",
    "# just_sent_messages = [' '.join(i) for i in filter_just_sent]# Reduce logging output.\n",
    "# with tf.Session() as session:\n",
    "#     session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#     sent_embeddings = session.run(embed(sent_messages))\n",
    "#     just_sent_embeddings = session.run(embed(just_sent_messages))\n",
    "# print(sent_embeddings[0],just_sent_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gensim\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('./dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "from gensim.models import Word2Vec\n",
    "from fse.models import Sentence2Vec\n",
    "\n",
    "# model = Word2Vec(filter_sent, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "sentences_emb = se.train(filter_sent)\n",
    "\n",
    "# model = Word2Vec(filter_just_sent, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "just_sentences_emb = se.train(filter_just_sent)\n",
    "\n",
    "# model = Word2Vec(meta1, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta1_emb = se.train(meta1)\n",
    "\n",
    "# model = Word2Vec(meta2, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta2_emb = se.train(meta2)\n",
    "\n",
    "# model = Word2Vec(meta3, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta3_emb = se.train(meta3)\n",
    "\n",
    "# model = Word2Vec(meta4, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta4_emb = se.train(meta4)\n",
    "\n",
    "# model = Word2Vec(meta5, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta5_emb = se.train(meta5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=[]\n",
    "for i in range(len(label)):\n",
    "    train_set.append(list(sentences_emb[i])+list(just_sentences_emb[i])+list(meta1_emb[i])+list(meta2_emb[i])+list(meta3_emb[i])+list(meta4_emb[i])+list(meta5_emb[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=['false','pants-fire','barely-true','half-true','mostly-true','true']\n",
    "train_labels=[]\n",
    "for i in label:\n",
    "    train_labels.append(labels.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For test set\n",
    "\n",
    "sent=[]\n",
    "just_sent=[]\n",
    "label=[]\n",
    "meta1=[]\n",
    "meta2=[]\n",
    "meta3=[]\n",
    "meta4=[]\n",
    "meta5=[]\n",
    "with open('./dataset/test2.tsv') as f:\n",
    "    lines=f.readlines()\n",
    "    for i in lines:\n",
    "        i=i.strip('\\n')\n",
    "        i=i.split('\\t')\n",
    "        if len(i[2].split(' '))<=2:\n",
    "            label.append(i[2])\n",
    "            sent.append(i[3])\n",
    "            meta1.append(i[4])\n",
    "            meta2.append(i[5])\n",
    "            meta3.append(i[6])\n",
    "            meta4.append(i[7])\n",
    "            meta5.append(i[-2])\n",
    "            just_sent.append(i[-1])\n",
    "#print(sent[0],just_sent[0],label[0],meta1[0],meta2[0],meta3[0],meta4[0],meta5[0])\n",
    "# for i in range(len(label)):\n",
    "#     print(i,label[i])\n",
    "filter_sent=[]\n",
    "filter_just_sent=[]\n",
    "for i in range(len(label)):\n",
    "    filter_sent.append(filter(sent[i]))\n",
    "    filter_just_sent.append(filter(just_sent[i]))\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "meta1=[word_tokenize(w.lower()) for w in meta1]\n",
    "meta2=[word_tokenize(w.lower()) for w in meta2]\n",
    "meta3=[word_tokenize(w.lower()) for w in meta5]\n",
    "meta4=[word_tokenize(w.lower()) for w in meta4]\n",
    "meta5=[word_tokenize(w.lower()) for w in meta5]\n",
    "\n",
    "# model = Word2Vec(filter_sent, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "sentences_emb = se.train(filter_sent)\n",
    "\n",
    "# model = Word2Vec(filter_just_sent, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "just_sentences_emb = se.train(filter_just_sent)\n",
    "\n",
    "# model = Word2Vec(meta1, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta1_emb = se.train(meta1)\n",
    "\n",
    "# model = Word2Vec(meta2, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta2_emb = se.train(meta2)\n",
    "\n",
    "# model = Word2Vec(meta3, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta3_emb = se.train(meta3)\n",
    "\n",
    "# model = Word2Vec(meta4, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta4_emb = se.train(meta4)\n",
    "\n",
    "# model = Word2Vec(meta5, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta5_emb = se.train(meta5)\n",
    "\n",
    "test_set=[]\n",
    "for i in range(len(label)):\n",
    "    test_set.append(list(sentences_emb[i])+list(just_sentences_emb[i])+list(meta1_emb[i])+list(meta2_emb[i])+list(meta3_emb[i])+list(meta4_emb[i])+list(meta5_emb[i]))\n",
    "labels=['false','pants-fire','barely-true','half-true','mostly-true','true']\n",
    "test_labels=[]\n",
    "for i in label:\n",
    "    test_labels.append(labels.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./dataset/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For validation set\n",
    "\n",
    "sent=[]\n",
    "just_sent=[]\n",
    "label=[]\n",
    "meta1=[]\n",
    "meta2=[]\n",
    "meta3=[]\n",
    "meta4=[]\n",
    "meta5=[]\n",
    "with open('./dataset/val2.tsv') as f:\n",
    "    lines=f.readlines()\n",
    "    for i in lines:\n",
    "        i=i.strip('\\n')\n",
    "        i=i.split('\\t')\n",
    "        if len(i[2].split(' '))<=2:\n",
    "            label.append(i[2])\n",
    "            sent.append(i[3])\n",
    "            meta1.append(i[4])\n",
    "            meta2.append(i[5])\n",
    "            meta3.append(i[6])\n",
    "            meta4.append(i[7])\n",
    "            meta5.append(i[-2])\n",
    "            just_sent.append(i[-1])\n",
    "#print(sent[0],just_sent[0],label[0],meta1[0],meta2[0],meta3[0],meta4[0],meta5[0])\n",
    "# for i in range(len(label)):\n",
    "#     print(i,label[i])\n",
    "filter_sent=[]\n",
    "filter_just_sent=[]\n",
    "for i in range(len(label)):\n",
    "    filter_sent.append(filter(sent[i]))\n",
    "    filter_just_sent.append(filter(just_sent[i]))\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "meta1=[word_tokenize(w.lower()) for w in meta1]\n",
    "meta2=[word_tokenize(w.lower()) for w in meta2]\n",
    "meta3=[word_tokenize(w.lower()) for w in meta5]\n",
    "meta4=[word_tokenize(w.lower()) for w in meta4]\n",
    "meta5=[word_tokenize(w.lower()) for w in meta5]\n",
    "\n",
    "model = Word2Vec(filter_sent, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "sentences_emb = se.train(filter_sent)\n",
    "\n",
    "model = Word2Vec(filter_just_sent, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "just_sentences_emb = se.train(filter_just_sent)\n",
    "\n",
    "model = Word2Vec(meta1, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta1_emb = se.train(meta1)\n",
    "\n",
    "model = Word2Vec(meta2, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta2_emb = se.train(meta2)\n",
    "\n",
    "model = Word2Vec(meta3, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta3_emb = se.train(meta3)\n",
    "\n",
    "model = Word2Vec(meta4, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta4_emb = se.train(meta4)\n",
    "\n",
    "model = Word2Vec(meta5, min_count=0)\n",
    "se = Sentence2Vec(model)\n",
    "meta5_emb = se.train(meta5)\n",
    "\n",
    "val_set=[]\n",
    "for i in range(len(label)):\n",
    "    val_set.append(list(sentences_emb[i])+list(just_sentences_emb[i])+list(meta1_emb[i])+list(meta2_emb[i])+list(meta3_emb[i])+list(meta4_emb[i])+list(meta5_emb[i]))\n",
    "labels=['false','pants-fire','barely-true','half-true','mostly-true','true']\n",
    "val_labels=[]\n",
    "for i in label:\n",
    "    val_labels.append(labels.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary: 43.646408839779006 Multi-Class: 19.652722967640095\n"
     ]
    }
   ],
   "source": [
    "# SVM with RBF kernel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# clf=AdaBoostClassifier()\n",
    "clf=SVC()\n",
    "clf.fit(train_set+val_set,train_labels+val_labels)\n",
    "prediction=clf.predict(test_set)\n",
    "binary=0\n",
    "multi=0\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i]==test_labels[i]:\n",
    "        multi+=1   \n",
    "    if prediction[i]//3==test_labels[i]//3:\n",
    "        binary+=1\n",
    "print(\"Binary:\",binary*100/len(prediction),\"Multi-Class:\",multi*100/len(prediction))\n",
    "# score=clf.score(test_set,test_labels)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0810 09:26:28.718624 140207072442112 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0810 09:26:28.721576 140207072442112 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0810 09:26:28.882823 140207072442112 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0810 09:26:28.893254 140207072442112 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0810 09:26:28.894232 140207072442112 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0810 09:26:29.208966 140207072442112 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0810 09:26:29.239620 140207072442112 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0810 09:26:29.272339 140207072442112 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0810 09:26:29.307059 140207072442112 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "W0810 09:26:29.726730 140207072442112 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "epochs = 10\n",
    "emb_dim = 700\n",
    "batch_size = 5000\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(700, emb_dim, input_length=len(train_set)))\n",
    "model.add(SpatialDropout1D(0.7))\n",
    "model.add(LSTM(200, dropout=0.7, recurrent_dropout=0.7))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "# history = model.fit(train_set+val_set, train_labels+val_labels, epochs=epochs,validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)])\n",
    "# pred=model.predict(test_set,test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
